{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1e7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import dataset,data_deal\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from models import *\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import random\n",
    "from TrainandTest import train_MLP,test_MLP\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def same_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7abae5",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59e2b25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "file = './data/total 12 data.xlsx'\n",
    "data = pd.read_excel(file,engine=\"openpyxl\")\n",
    "data = np.array(data)\n",
    "l = len(data)\n",
    "para_path = './save/parameter'\n",
    "batch_size = 300\n",
    "plot = True\n",
    "# np.set_printoptions(threshold=np.sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc2a21",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8231b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complexity(model):\n",
    "    param_squared_sums = {}\n",
    "    encoder_parameters = list(filter(lambda x: x[0].startswith('encoder'),list(model.named_parameters())))\n",
    "    for name, param in encoder_parameters:\n",
    "#         print(param)\n",
    "        if 'weight' in name:\n",
    "            param_squared_sums[name] = torch.sum(param ** 2)\n",
    "    return sum(param_squared_sums.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99a53e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "flag =  0\n",
    "Type = 'even'\n",
    "N_size = '<=126'\n",
    "lr = 0.1\n",
    "k = 10\n",
    "alpha = 0.1\n",
    "input_size = 9\n",
    "epsilon = 0\n",
    "seed1,seed2 = 3063344997,173232029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e24643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=k ,shuffle=True,random_state=seed1)\n",
    "\n",
    "total_data = dataset(data,Type,N_size,input_size)\n",
    "total_data = np.array(total_data,dtype = object)\n",
    "\n",
    "X = total_data[:,0]\n",
    "y = total_data[:,1].astype(np.float32)\n",
    "\n",
    "temp_loss = 0\n",
    "l1= X[0].shape[0]\n",
    "l2 =len(X)\n",
    "results_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f02555dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ae init weight 12.281000137329102\n",
      "mlp init weight 9.813122749328613\n",
      "ae test loss:0.659867525100708\n",
      "mlp test loss:0.5233104825019836\n",
      "ae trained weight -17.4729061126709\n",
      "mlp trained weight 30.679094314575195\n",
      "\n",
      "ae init weight 6.0461225509643555\n",
      "mlp init weight 11.666089057922363\n",
      "ae test loss:0.5129931569099426\n",
      "mlp test loss:0.5225540995597839\n",
      "ae trained weight 3.7200927734375\n",
      "mlp trained weight 34.38715362548828\n",
      "\n",
      "ae init weight 2.364917516708374\n",
      "mlp init weight 7.701430320739746\n",
      "ae test loss:0.10078362375497818\n",
      "mlp test loss:0.07697702944278717\n",
      "ae trained weight -4.682962417602539\n",
      "mlp trained weight -19.521282196044922\n",
      "\n",
      "ae init weight 5.264561653137207\n",
      "mlp init weight 2.989790678024292\n",
      "ae test loss:0.8276804089546204\n",
      "mlp test loss:0.5544532537460327\n",
      "ae trained weight -6.412235260009766\n",
      "mlp trained weight -5.673384666442871\n",
      "\n",
      "ae init weight 6.814538955688477\n",
      "mlp init weight 7.049803733825684\n",
      "ae test loss:0.4760608673095703\n",
      "mlp test loss:0.7388693690299988\n",
      "ae trained weight -7.89101505279541\n",
      "mlp trained weight -6.804690361022949\n",
      "\n",
      "ae init weight 2.3231301307678223\n",
      "mlp init weight 8.317192077636719\n",
      "ae test loss:0.5607417821884155\n",
      "mlp test loss:0.3637171983718872\n",
      "ae trained weight -17.357622146606445\n",
      "mlp trained weight -26.674423217773438\n",
      "\n",
      "ae init weight 8.253832817077637\n",
      "mlp init weight 4.030355930328369\n",
      "ae test loss:0.3945061266422272\n",
      "mlp test loss:0.21770724654197693\n",
      "ae trained weight 16.233083724975586\n",
      "mlp trained weight 11.476516723632812\n",
      "\n",
      "ae init weight 6.860579013824463\n",
      "mlp init weight 13.096900939941406\n",
      "ae test loss:0.7353528738021851\n",
      "mlp test loss:0.5051242113113403\n",
      "ae trained weight 2.8317861557006836\n",
      "mlp trained weight 36.09799575805664\n",
      "\n",
      "ae init weight 9.365303039550781\n",
      "mlp init weight 5.051427841186523\n",
      "ae test loss:0.14223912358283997\n",
      "mlp test loss:0.21040531992912292\n",
      "ae trained weight 22.93911361694336\n",
      "mlp trained weight 21.437637329101562\n",
      "\n",
      "ae init weight 15.35062313079834\n",
      "mlp init weight 12.889232635498047\n",
      "ae test loss:0.24119170010089874\n",
      "mlp test loss:1.5105700492858887\n",
      "ae trained weight 2.0011215209960938\n",
      "mlp trained weight -0.22127246856689453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    ae_model = get_models(\"AE_MLP\",input_size,epsilon)\n",
    "    mlp_model = get_models(\"MLP\",input_size,epsilon)\n",
    "    print(f\"ae init weight {complexity(ae_model)}\")\n",
    "    print(f\"mlp init weight {complexity(mlp_model)}\")\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train,X_test,y_train,y_test,scaler = data_deal(X_train,X_test,y_train,y_test,scaler,input_size,flag)\n",
    "    train_MLP(X_train,y_train,ae_model,lr,alpha,fold,para_path,flag)\n",
    "    loss,_ = test_MLP(X_test,y_test,ae_model,fold,para_path,scaler,results_df,l1,flag)\n",
    "    print(f\"ae test loss:{loss.item()}\")\n",
    "    train_MLP(X_train,y_train,mlp_model,lr,alpha,fold,para_path,flag)\n",
    "    loss,_ = test_MLP(X_test,y_test,mlp_model,fold,para_path,scaler,results_df,l1,flag)\n",
    "    print(f\"mlp test loss:{loss.item()}\")\n",
    "    print(f\"ae trained weight {complexity(ae_model)}\")\n",
    "    print(f\"mlp trained weight {complexity(mlp_model)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbccd29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
