{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd1e7eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import dataset,get_Kfold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from models import MLP\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import random\n",
    "from TrainandTest import train_MLP,test_MLP\n",
    "\n",
    "def same_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "same_seeds(114514)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5366db30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models(model):\n",
    "    if model=='MLP':\n",
    "        return MLP(5,64)\n",
    "\n",
    "def train_epoch(train_loader,model,criterion,optimizer,device):\n",
    "    losses = []\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        labels = labels.to(device)\n",
    "        inputs = inputs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    losses.append(loss.item())\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9ec5647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------[ K:{k_step+1} Type:{Type} N_size:{N_size} ]-----------\n",
      "loss:6.9553070068359375 saving...\n",
      "loss:1.6970878839492798 saving...\n",
      "loss:1.6833913326263428 saving...\n",
      "loss:1.5328588485717773 saving...\n",
      "loss:0.9129649996757507 saving...\n",
      "loss:0.7964922189712524 saving...\n",
      "loss:0.612503707408905 saving...\n",
      "loss:0.5151007771492004 saving...\n",
      "loss:0.33268097043037415 saving...\n",
      "loss:0.11701059341430664 saving...\n",
      "loss:0.06782921403646469 saving...\n",
      "loss:0.020726999267935753 saving...\n",
      "loss:0.017559058964252472 saving...\n",
      "loss:0.016195999458432198 saving...\n",
      "loss:0.015904607251286507 saving...\n",
      "loss:0.00934223085641861 saving...\n",
      "loss:0.004964521620422602 saving...\n",
      "loss:0.00226382864639163 saving...\n",
      "loss:0.0009549474925734103 saving...\n",
      "loss:0.0008553307852707803 saving...\n",
      "loss:0.0008150510839186609 saving...\n",
      "loss:0.0007965481490828097 saving...\n",
      "loss:0.00040174214518629014 saving...\n",
      "loss:0.0002625538036227226 saving...\n",
      "loss:0.0002505354641471058 saving...\n",
      "loss:0.00024850599584169686 saving...\n",
      "loss:0.00021726849081460387 saving...\n",
      "loss:0.0001934614556375891 saving...\n",
      "loss:0.0001774752017809078 saving...\n",
      "loss:0.0001670893543632701 saving...\n",
      "loss:0.00015151668048929423 saving...\n",
      "loss:0.00014741263294126838 saving...\n",
      "loss:0.0001429197145625949 saving...\n",
      "loss:0.00013877016317564994 saving...\n",
      "loss:0.00013843836495652795 saving...\n",
      "loss:0.00013723355368711054 saving...\n",
      "loss:0.0001339568116236478 saving...\n",
      "loss:0.0001310414809267968 saving...\n",
      "loss:0.0001290244108531624 saving...\n",
      "loss:0.00012869515921920538 saving...\n",
      "loss:0.00012723746476694942 saving...\n",
      "loss:0.0001251734356628731 saving...\n",
      "loss:0.00012101661559427157 saving...\n",
      "loss:0.00011757756146835163 saving...\n",
      "loss:0.00011505366273922846 saving...\n",
      "loss:0.00011435680062277243 saving...\n",
      "loss:0.00011391013686079532 saving...\n",
      "loss:0.00011231273674638942 saving...\n",
      "loss:0.00010907539399340749 saving...\n",
      "loss:0.00010634586942614987 saving...\n",
      "loss:0.00010399691382190213 saving...\n",
      "loss:0.00010296960681444034 saving...\n",
      "loss:0.00010226167796645314 saving...\n",
      "loss:0.0001011433923849836 saving...\n",
      "loss:9.988747478928417e-05 saving...\n",
      "loss:9.779797255760059e-05 saving...\n",
      "loss:9.619361662771553e-05 saving...\n",
      "loss:9.449494245927781e-05 saving...\n",
      "loss:9.320479875896126e-05 saving...\n",
      "loss:9.18621153687127e-05 saving...\n",
      "loss:9.040670556714758e-05 saving...\n",
      "loss:8.896919462131336e-05 saving...\n",
      "loss:8.747104584472254e-05 saving...\n",
      "loss:8.636195707367733e-05 saving...\n",
      "loss:8.502764831064269e-05 saving...\n",
      "loss:8.385487308260053e-05 saving...\n",
      "loss:8.243766205850989e-05 saving...\n",
      "loss:8.0910962424241e-05 saving...\n",
      "loss:7.953256863402203e-05 saving...\n",
      "loss:7.821665349183604e-05 saving...\n",
      "loss:7.710299541940913e-05 saving...\n",
      "loss:7.599309174111113e-05 saving...\n",
      "loss:7.48188904253766e-05 saving...\n",
      "loss:7.355525303864852e-05 saving...\n",
      "loss:7.224622095236555e-05 saving...\n",
      "loss:7.101023948052898e-05 saving...\n",
      "loss:6.982715422054753e-05 saving...\n",
      "loss:6.873718666611239e-05 saving...\n",
      "loss:6.765494617866352e-05 saving...\n",
      "loss:6.648036651313305e-05 saving...\n",
      "loss:6.539198511745781e-05 saving...\n",
      "loss:6.422788283089176e-05 saving...\n",
      "loss:6.315085920505226e-05 saving...\n",
      "loss:6.211240543052554e-05 saving...\n",
      "loss:6.096801007515751e-05 saving...\n",
      "loss:5.991488433210179e-05 saving...\n",
      "loss:5.88483308092691e-05 saving...\n",
      "loss:5.7800021750153974e-05 saving...\n",
      "loss:5.681944821844809e-05 saving...\n",
      "loss:5.579289427259937e-05 saving...\n",
      "loss:5.48209973203484e-05 saving...\n",
      "loss:5.383993629948236e-05 saving...\n",
      "loss:5.280476761981845e-05 saving...\n",
      "loss:5.185918416827917e-05 saving...\n",
      "loss:5.0927385018439963e-05 saving...\n",
      "loss:4.995639392291196e-05 saving...\n",
      "loss:4.906478352495469e-05 saving...\n",
      "loss:4.8163416067836806e-05 saving...\n",
      "loss:4.721426739706658e-05 saving...\n",
      "loss:4.631670890375972e-05 saving...\n",
      "loss:4.544845432974398e-05 saving...\n",
      "loss:4.455536327441223e-05 saving...\n",
      "loss:4.372688272269443e-05 saving...\n",
      "loss:4.28659186582081e-05 saving...\n",
      "loss:4.202223499305546e-05 saving...\n",
      "loss:4.122367317904718e-05 saving...\n",
      "loss:4.0382950828643516e-05 saving...\n",
      "loss:3.958389424951747e-05 saving...\n",
      "loss:3.880762233166024e-05 saving...\n",
      "loss:3.8004738598829135e-05 saving...\n",
      "loss:3.725354326888919e-05 saving...\n",
      "loss:3.650117650977336e-05 saving...\n",
      "loss:3.5732165997615084e-05 saving...\n",
      "loss:3.5043856769334525e-05 saving...\n",
      "loss:3.429678690736182e-05 saving...\n",
      "loss:3.356165689183399e-05 saving...\n",
      "loss:3.2898311474127695e-05 saving...\n",
      "loss:3.217062112526037e-05 saving...\n",
      "loss:3.149333133478649e-05 saving...\n",
      "loss:3.085561911575496e-05 saving...\n",
      "loss:3.0151331884553656e-05 saving...\n",
      "loss:2.951336864498444e-05 saving...\n",
      "loss:2.8870896130683832e-05 saving...\n",
      "loss:2.8228694645804353e-05 saving...\n",
      "loss:2.7618456442723982e-05 saving...\n",
      "loss:2.6996232918463647e-05 saving...\n",
      "loss:2.6425550458952785e-05 saving...\n",
      "loss:2.580666114226915e-05 saving...\n",
      "loss:2.524756928323768e-05 saving...\n",
      "loss:2.467124431859702e-05 saving...\n",
      "loss:2.4098744688672014e-05 saving...\n",
      "loss:2.3569687982671894e-05 saving...\n",
      "loss:2.3000746296020225e-05 saving...\n",
      "loss:2.249334465886932e-05 saving...\n",
      "loss:2.1957015633233823e-05 saving...\n",
      "loss:2.143858182535041e-05 saving...\n",
      "loss:2.0952607883373275e-05 saving...\n",
      "loss:2.0432949895621277e-05 saving...\n",
      "loss:1.996182072616648e-05 saving...\n",
      "loss:1.9483044525259174e-05 saving...\n",
      "loss:1.899970811791718e-05 saving...\n",
      "loss:1.8600185285322368e-05 saving...\n",
      "loss:1.8176842786488123e-05 saving...\n",
      "loss:1.7648233551881276e-05 saving...\n",
      "loss:1.731023075990379e-05 saving...\n",
      "loss:1.7000897059915587e-05 saving...\n",
      "loss:1.6436513760709204e-05 saving...\n",
      "loss:1.5959294614731334e-05 saving...\n",
      "loss:1.560393320687581e-05 saving...\n",
      "loss:1.5200094821921084e-05 saving...\n",
      "loss:1.4786972315050662e-05 saving...\n",
      "loss:1.4398378880287055e-05 saving...\n",
      "loss:1.4039638699614443e-05 saving...\n",
      "loss:1.3718598893319722e-05 saving...\n",
      "loss:1.3363502148422413e-05 saving...\n",
      "loss:1.2984528439119458e-05 saving...\n",
      "loss:1.2630747733055614e-05 saving...\n",
      "loss:1.2299490663281176e-05 saving...\n",
      "loss:1.1981268471572548e-05 saving...\n",
      "loss:1.1673036169668194e-05 saving...\n",
      "loss:1.1371003893145826e-05 saving...\n",
      "loss:1.1061900295317173e-05 saving...\n",
      "loss:1.0753982678579632e-05 saving...\n",
      "loss:1.0454320545250084e-05 saving...\n",
      "loss:1.0168027984036598e-05 saving...\n",
      "loss:9.894453796732705e-06 saving...\n",
      "loss:9.61977093538735e-06 saving...\n",
      "loss:9.342941666545812e-06 saving...\n",
      "loss:9.079306437342893e-06 saving...\n",
      "loss:8.837913810566533e-06 saving...\n",
      "loss:8.60254749568412e-06 saving...\n",
      "loss:8.3476315921871e-06 saving...\n",
      "loss:8.09622542874422e-06 saving...\n",
      "loss:7.861206540837884e-06 saving...\n",
      "loss:7.636929694854189e-06 saving...\n",
      "loss:7.418439054163173e-06 saving...\n",
      "loss:7.201930657174671e-06 saving...\n",
      "loss:6.977709745115135e-06 saving...\n",
      "loss:6.7964388108521234e-06 saving...\n",
      "loss:6.672144991171081e-06 saving...\n",
      "loss:6.471371307270601e-06 saving...\n",
      "loss:6.22841344011249e-06 saving...\n",
      "loss:6.0019197007932235e-06 saving...\n",
      "loss:5.866212632099632e-06 saving...\n",
      "loss:5.811484697915148e-06 saving...\n",
      "loss:5.615575446427101e-06 saving...\n",
      "loss:5.337348284228938e-06 saving...\n",
      "loss:5.144515853316989e-06 saving...\n",
      "loss:5.060483090346679e-06 saving...\n",
      "loss:4.9761933951231185e-06 saving...\n",
      "loss:4.765803168993443e-06 saving...\n",
      "loss:4.538855137070641e-06 saving...\n",
      "loss:4.411733698361786e-06 saving...\n",
      "loss:4.373406227387022e-06 saving...\n",
      "loss:4.276802883396158e-06 saving...\n",
      "loss:4.060428182128817e-06 saving...\n",
      "loss:3.867189661832526e-06 saving...\n",
      "loss:3.7346583212638507e-06 saving...\n",
      "loss:3.6226242627890315e-06 saving...\n",
      "loss:3.501234232317074e-06 saving...\n",
      "loss:3.3817907478805864e-06 saving...\n",
      "loss:3.272373305662768e-06 saving...\n",
      "loss:3.1646800380258355e-06 saving...\n",
      "loss:3.058779611819773e-06 saving...\n",
      "loss:2.9580132832052186e-06 saving...\n",
      "loss:2.8592917260539252e-06 saving...\n",
      "loss:2.7657656573865097e-06 saving...\n",
      "loss:2.6758389140013605e-06 saving...\n",
      "loss:2.580776936156326e-06 saving...\n",
      "loss:2.5006918349390617e-06 saving...\n",
      "loss:2.4236298941104906e-06 saving...\n",
      "loss:2.3315290036407532e-06 saving...\n",
      "loss:2.246661779281567e-06 saving...\n",
      "loss:2.1710382043238496e-06 saving...\n",
      "loss:2.100276560668135e-06 saving...\n",
      "loss:2.0276820578146726e-06 saving...\n",
      "loss:1.9533767954271752e-06 saving...\n",
      "loss:1.8844046962840366e-06 saving...\n",
      "loss:1.8204233356300392e-06 saving...\n",
      "loss:1.7582982536623604e-06 saving...\n",
      "loss:1.6939029592322186e-06 saving...\n",
      "loss:1.6306047427860904e-06 saving...\n",
      "loss:1.5752715398775763e-06 saving...\n",
      "loss:1.5307077774195932e-06 saving...\n",
      "loss:1.475315002608113e-06 saving...\n",
      "loss:1.4115973954176297e-06 saving...\n",
      "loss:1.358046233690402e-06 saving...\n",
      "loss:1.3181471558709745e-06 saving...\n",
      "loss:1.2783572174157598e-06 saving...\n",
      "loss:1.2228061905261711e-06 saving...\n",
      "loss:1.1702016990966513e-06 saving...\n",
      "loss:1.1279020100118942e-06 saving...\n",
      "loss:1.0972295285682776e-06 saving...\n",
      "loss:1.0620645980452537e-06 saving...\n",
      "loss:1.0105309229402337e-06 saving...\n",
      "loss:9.676188028606703e-07 saving...\n",
      "loss:9.33107401124289e-07 saving...\n",
      "loss:8.994203994916461e-07 saving...\n",
      "loss:8.638392614557233e-07 saving...\n",
      "loss:8.29120040179987e-07 saving...\n",
      "loss:7.986795367287414e-07 saving...\n",
      "loss:7.689276912969945e-07 saving...\n",
      "loss:7.374338792942581e-07 saving...\n",
      "loss:7.116760230019281e-07 saving...\n",
      "loss:6.864079864499217e-07 saving...\n",
      "loss:6.558447012139368e-07 saving...\n",
      "loss:6.31112641258369e-07 saving...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:6.086298753871233e-07 saving...\n",
      "loss:5.819295552100812e-07 saving...\n",
      "loss:5.591409717453644e-07 saving...\n",
      "loss:5.391007675825676e-07 saving...\n",
      "loss:5.163385026207834e-07 saving...\n",
      "loss:4.946339799971611e-07 saving...\n",
      "loss:4.7511585421489144e-07 saving...\n",
      "loss:4.557957993256423e-07 saving...\n",
      "loss:4.3888167056138627e-07 saving...\n",
      "loss:4.2194923821625707e-07 saving...\n",
      "loss:4.029421347695461e-07 saving...\n",
      "loss:3.8922283351894293e-07 saving...\n",
      "loss:3.751608517177374e-07 saving...\n",
      "loss:3.5583596513788507e-07 saving...\n",
      "loss:3.447986216542631e-07 saving...\n",
      "loss:3.3239447816413303e-07 saving...\n",
      "loss:3.13764957127205e-07 saving...\n",
      "loss:3.044533229967783e-07 saving...\n",
      "loss:2.944957202544174e-07 saving...\n",
      "loss:2.7670969870996487e-07 saving...\n",
      "loss:2.690838130092743e-07 saving...\n",
      "loss:2.6496417149246554e-07 saving...\n",
      "loss:2.479471845617809e-07 saving...\n",
      "loss:2.3311250174629095e-07 saving...\n",
      "loss:2.326241173022936e-07 saving...\n",
      "loss:2.274564394610934e-07 saving...\n",
      "loss:2.063671473706563e-07 saving...\n",
      "loss:1.8754988673208572e-07 saving...\n",
      "loss:1.858762885831311e-07 saving...\n",
      "loss:1.5929322216834407e-07 saving...\n",
      "loss:1.439182852891463e-07 saving...\n",
      "loss:1.4018633009982295e-07 saving...\n",
      "loss:1.2699813112249103e-07 saving...\n",
      "loss:1.2182559316897823e-07 saving...\n",
      "loss:1.1693647650190542e-07 saving...\n",
      "loss:1.0635861968921745e-07 saving...\n",
      "loss:1.01466191892996e-07 saving...\n",
      "loss:9.937765099721219e-08 saving...\n",
      "loss:9.215563068210031e-08 saving...\n",
      "loss:8.817749375111816e-08 saving...\n",
      "loss:8.552480323942291e-08 saving...\n",
      "loss:8.031393150531585e-08 saving...\n",
      "loss:7.654529099454521e-08 saving...\n",
      "loss:7.512830535461035e-08 saving...\n",
      "loss:7.131672674631773e-08 saving...\n",
      "loss:6.634495264279394e-08 saving...\n",
      "loss:6.440045297040342e-08 saving...\n",
      "loss:5.958789373039508e-08 saving...\n",
      "loss:5.41237383799853e-08 saving...\n",
      "loss:5.0231985682103186e-08 saving...\n",
      "loss:4.788443064285275e-08 saving...\n",
      "loss:4.7255792168243715e-08 saving...\n",
      "loss:4.585749735497302e-08 saving...\n",
      "loss:3.9689030728595753e-08 saving...\n",
      "loss:3.095759382176766e-08 saving...\n",
      "loss:4.29732027740215e-09 saving...\n",
      "loss:7.928159795334011e-10 saving...\n",
      "loss:1.0784106940775473e-10 saving...\n",
      "loss:6.154134146729717e-11 saving...\n",
      "loss:3.233235901234366e-11 saving...\n",
      "loss:1.5920851442752237e-11 saving...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "linear(): argument 'input' (position 1) must be Tensor, not int",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (inputs, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_loader):\n\u001b[0;32m     37\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels \n\u001b[1;32m---> 38\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mprint\u001b[39m(labels,outputs)\n\u001b[0;32m     40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\py3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\2023-deep-physics-main\\models.py:17\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 17\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfnn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\py3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\py3.9\\lib\\site-packages\\torch\\nn\\modules\\container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    203\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 204\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\py3.9\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\py3.9\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: linear(): argument 'input' (position 1) must be Tensor, not int"
     ]
    }
   ],
   "source": [
    "with open('config.json','r',encoding='utf-8')as f:\n",
    "    config_list = json.load(f)\n",
    "#device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu'\n",
    "file = './data/jiji small126.xlsx'\n",
    "batch_size = 300\n",
    "\n",
    "for config in config_list:\n",
    "    Type = config['Type']\n",
    "    N_size = config['N_size']\n",
    "    num_epoch = config['num_epoch']\n",
    "    lr = config['lr']\n",
    "    k = config['k']\n",
    "    alpha,beta = config['alpha'],config['beta']\n",
    "    \n",
    "    K_fold_data = get_Kfold(file,2)\n",
    "    for k_step,(trian_data,test_data) in enumerate(K_fold_data):\n",
    "        print(\"-----------[ K:{k_step+1} Type:{Type} N_size:{N_size} ]-----------\")\n",
    "        model = get_models(config['model']).to(device)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(),lr=lr)\n",
    "        lr_scheduler = optim.lr_scheduler.LambdaLR(optimizer,lambda x: 1 - x/num_epoch,last_epoch=-1)\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        trian_data = dataset(trian_data,Type,N_size)\n",
    "        test_data = dataset(test_data,Type,N_size)\n",
    " #       trian_data = scaler.fit_transform(trian_data)\n",
    " #       test_data = scaler.transform(test_data)\n",
    "        train_loader = DataLoader(dataset=trian_data,batch_size=batch_size)\n",
    "        test_loader = DataLoader(dataset=test_data,batch_size=batch_size)\n",
    "        train_MLP(train_loader=train_loader,model=model,criterion=criterion,optimizer=optimizer,epoch = 1000,fold = k_step,lr_scheduler=lr_scheduler)\n",
    "        model.load_state_dict(torch.load(f'./model_fold_{k_step + 1}.ckpt'))\n",
    "        model.eval()\n",
    "        test_losses = []\n",
    "        with torch.no_grad():  \n",
    "            for (inputs, labels) in enumerate(test_loader):\n",
    "                labels = labels \n",
    "                outputs = model(inputs)\n",
    "                print(labels,outputs)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            test_losses.append(loss.item())\n",
    "            test_loss = sum(test_losses)/len(test_losses)\n",
    "            rmse = test_loss**0.5\n",
    "            print(f'Test Loss: {test_loss:.10f}, Test RMSE: {rmse:.10f}') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1fb4aa7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0004652027855627239]\n"
     ]
    }
   ],
   "source": [
    "print(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153dfa1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
